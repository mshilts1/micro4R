---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# micro4R

Microbiome data analysis tools for R

<!-- badges: start -->
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)
[![CRAN status](https://www.r-pkg.org/badges/version/micro4R)](https://CRAN.R-project.org/)
[![Codecov test coverage](https://codecov.io/gh/mshilts1/micro4R/graph/badge.svg)](https://app.codecov.io/gh/mshilts1/micro4R)
[![R-CMD-check](https://github.com/mshilts1/micro4R/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/mshilts1/micro4R/actions/workflows/R-CMD-check.yaml)
<!-- badges: end -->



The goal of `micro4R` was to create an R package for microbiome data processing with a low barrier to entry. I started my career in microbiome research at the bench and had to [ELI5](https://media0.giphy.com/media/v1.Y2lkPTc5MGI3NjExY3hrYzg1a2I2eGtuNWIwYTRqNDMzNGE0cWlkNGE5OXB4ZHV1YXY4dCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/WsNbxuFkLi3IuGI9NU/giphy.gif) to myself how to process and analyze "big data". I've spent a ton of time poring over and experimenting with [others' code](https://github.com/mshilts1/micro4R?tab=readme-ov-file#acknowledgements), and want to pass it on.     <a href="https://mshilts1.github.io/micro4R/"><img src="man/figures/logo.png" align="right" height="139" alt="micro4R website" /></a>         

Likely, the ideal person to benefit from `micro4R` would be a bench scientist without much formal statistics or bioinformatics training. Fair warning, if you already have a strong stats/informatics background, this may not be of much use for you!            

This package does not create any brand new functionality and is essentially inspired by and/or a wrapper of existing tools [others](https://github.com/mshilts1/micro4R?tab=readme-ov-file#acknowledgements) have already created. Much of what it does can be accomplished with other packages, such as [phyloseq](https://bioconductor.org/packages/release/bioc/html/phyloseq.html), [QIIME 2](https://qiime2.org), and [MicrobiomeAnalyst](https://www.microbiomeanalyst.ca). One of these may be better for your purposes, and I'd encourage anyone new to the field to explore multiple tools!    



## Installation

All you really need is [R](https://cran.rstudio.com), but I'd recommend also downloading and working in [RStudio](https://posit.co/download/rstudio-desktop/). If you're a true newbie to R, there's tons of free [content](https://www.reddit.com/user/jjkraker/comments/zfhe1e/i_want_to_learn_basics_of_r_if_so_heres_a_reading/) to help you learn the basics.   

Once you're set in R/RStudio, you can install the development version of micro4R like so:

``` r
# install.packages("pak")
pak::pak("mshilts1/micro4R")
```


## Example

I'll run through a very small and simple possible use case below. For more detailed help and documentation, please explore the vignettes *(TBA)*.   

Included with the package is an extremely and unnaturally tiny toy example to demonstrate its major functionality, using subsampled publicly available nasal swab 16S microbiome [data](https://www.ncbi.nlm.nih.gov/bioproject/PRJNA726992) that I generated along with many [colleagues](https://pmc.ncbi.nlm.nih.gov/articles/PMC8819187/).   

### Making the ASV count and taxonomy tables

If you don't know what an ASV (Amplicon Sequence Variant) is, please go [here](https://benjjneb.github.io/dada2/index.html) first.    

The first thing we'll do on these files is run `dada2_asvtable()`, which is essentially a wrapper to generate an ASV count table by following a workflow similar to the [dada2 tutorial](https://benjjneb.github.io/dada2/tutorial.html). 

This function can take a number of arguments, but the most important one is 'where', which is the path to the folder where your FASTQ files are located.  

For demonstration purposes, it's been set to the relative path of the example FASTQ files that are included with the package:


```{r example1}
library(micro4R)

asvtable <- dada2_asvtable(where = "inst/extdata/f", chatty = FALSE, logfile = FALSE)
```


If you're running this with your own data, set 'where' to the path of the folder where your FASTQ files are stored. If you leave it empty (e.g., run `dada2_asvtable()`), it will default to searching in your current working directory. ('chatty' was set to FALSE because tons of information gets printed to the console otherwise; I'd recommend setting it to TRUE (the default) when you're processing data for real, as the information is  useful, but just too much here.)   

Let's take a quick look at what this asvtable looks like (using the `tibble::as_tibble()` function so it prints more nicely):


```{r example2}
tibble::as_tibble(asvtable, rownames = "SampleID")
```

This is basically just a count table of the number of ASVs detected in each sample.         

Let's look what the column names (AKA the names of the ASVs) look like:

```{r example2.1}
colnames(asvtable)
```

Our ASVs are by default just named after their literal DNA sequences. Since you're (probably?) not a computer, a string of hundreds of nucletotides is likely not something you can make much sense of by yourself. The next step will take those nucleotide sequences and compare them against a database (or two) of sequences with known taxonomy: 

```{r example3}
train <- "inst/extdata/db/EXAMPLE_silva_nr99_v138.2_toGenus_trainset.fa.gz" # set training database
species <- "inst/extdata/db/EXAMPLE_silva_v138.2_assignSpecies.fa.gz" # set species database

taxa <- dada2_taxa(asvtable = asvtable, train = train, species = species, chatty = FALSE)
```

There are two databases that we're using for taxonomic assignment here:    
1. 'train' needs to be the path to whatever database you'd like to use as the "training set of reference sequences with known taxonomy".   
2. 'species' is OPTIONAL. If you'd like to use this option, provide the path to a specifically formatted species assignment database. (Read more [here](https://benjjneb.github.io/dada2/assign.html#species-assignment).)

**CAUTION: The two databases used in the example here are comically small and artificial subsamples of the real SILVA databases, and should only ever be used for testing and demonstration purposes!** You'll definitely need to download and use the real [databases](https://benjjneb.github.io/dada2/training.html) for your actual data!       

There are many options for taxonomic databases you can use; the major players are SILVA, RDP, GreenGenes, and UNITE. Please go [here](https://benjjneb.github.io/dada2/training.html) for details and links. I usually prefer the SILVA databases, but you don't have to!

```{r example3.1, echo = FALSE}
# taxa <- tibble::as_tibble(taxa, rownames = "ASV")
```

Let's take a look at the taxonomy assignment table:

```{r example3.2, echo = FALSE}
tibble::as_tibble(taxa, rownames = "ASV")
```

It's a bit squished, but you can see this information is more human-friendly here. Each ASV has been given a taxonomic assignment the lowest taxonomic level the taxonomy assigner was confident of.    

### Sample metadata

Next, we need to load in some metadata about our samples. 

```{r example4}
metadata <- example_metadata()

metadata
```

Let's look at the 'SampleID' field, which is what is sound like, and uniquely identifies each sample:

```{r example 4.1}
metadata$SampleID
```

The first thing you may notice is the 'SampleIDs' here are the kinds of IDs that only a computer could love. For my standard workflow, I like to keep the SampleIDs as the FASTQ file names because they will by default automatically match the SampleIDs generated with dada2_asvtable().     

In this example, there is also a 'LabID' field, which is an ID that could have been used all through specimen processing, as it is much more human-friendly: 

```{r example 4.2}
metadata$LabID
```

To seemlessly use this package, you MUST have a column called 'SampleID', and those IDs must exactly match between your metadata and ASV count table objects. But otherwise, you're free to name your samples whatever you want.

What kind of information you'll need to have in your metadata object is HIGHLY dependent on your study, but there's some information that we must have for the optional (but highly recommended!) processing of your ASV table through [decontam](https://github.com/benjjneb/decontam).    

What does `decontam` need to know?    
1. To use the "prevalence" method: which samples are the negative controls.    
2. To use the "frequency" method: the DNA concentration in each sample prior to sequencing.    
3. To use both/either of the "prevalence" and "frequency" methods, you need all the above.    

Don't have any negative controls? You won't be able to run `decontam`, and I strongly recommend you include some next time! Both negative and positive controls are very important! Read more here: ^[1](https://pubmed.ncbi.nlm.nih.gov/25387460/),^ ^[2](https://pubmed.ncbi.nlm.nih.gov/27239228/),^ ^[3](https://bmcmicrobiol.biomedcentral.com/articles/10.1186/s12866-020-01839-y),^ ^[4](https://bmcmicrobiol.biomedcentral.com/articles/10.1186/s12866-016-0738-z),^ ^[5](https://journals.asm.org/doi/10.1128/msystems.00062-16)^  

In our example data, we only are able to use the "prevalence" method, because we know which samples were negative controls, and which were not. That information is in our column called "neg", where TRUE means it was a negative control:
```{r example 4.3}
metadata$neg
```

Next, let's run the `decontam_wrapper()` on the example data we've generated so far:
```{r example5}
decontam_wrapper(asvtable = asvtable, taxa = taxa, metadata = metadata, logfile = FALSE)
```

You'll see several messages, including one at the bottom that tells us "No contaminants were detected. Exiting function." With real data, you should not expect to ever see this message. Our example data set was just too stupidly small for `decontam` to work properly.     

So that I can demonstrate `decontam` actually doing something, we'll deliberately "contaminate" the ASV table with the `contaminate()` command. All we're doing is artificially adding counts to one of the ASVs, and throwing in a few more negative controls for good measure. This data is even more fake and made up than it was previously! The command `contaminate()` will also simultaneously create a matched metadata object with the additional made up negative controls.

```{r example5.1}
contaminated_asvtable <- converter(contaminate()$asvtable)
taxa <- dada2_taxa(asvtable = contaminated_asvtable, train = train, species = species) # we don't actually have to re-run this command with this specific example, but it's good practice to always ensure your asvtable and taxa tables match.
contaminated_metadata <- contaminate()$metadata
decontaminated <- decontam_wrapper(asvtable = contaminated_asvtable, taxa = taxa, metadata = contaminated_metadata, logfile = FALSE)
```

The object "decontaminated" contains a list of the... decontaminated... ASV table, taxa table, and for our convenience also includes the metadata table.

```{r example 5.2}
dim(taxa)
dim(decontaminated$taxa)
```
___



Move information to the bottom for anyone who wants more details

subsampled FASTQ files from a [manuscript](https://pmc.ncbi.nlm.nih.gov/articles/PMC8819187/) I co-authored with my colleagues, for which the raw data is publicly available under bioproject ID [PRJNA726992](https://www.ncbi.nlm.nih.gov/bioproject/PRJNA726992). From seven samples from this study, using [seqtk](https://github.com/lh3/seqtk), I randomly sampled **only 50 reads** from each FASTQ file so that the files  would take up minimal space and the example would run quickly.

If you'd like to run through this the full fastq files can be downloaded from SRA or as a zipped bolus [here](https://drive.google.com/file/d/1NOvmsxFxWb1Vigq8rdb5SCfLLNu-Qjy8/view?usp=sharing) 

Here is some body text that needs a footnote.^[This is the content of the footnote.]

* logo made by me using artwork from [Canva](https://www.canva.com/) (©[iconbunny11](https://www.canva.com/p/id/BAClqvm1MBE/)) followed by [hexSticker](https://github.com/GuangchuangYu/hexSticker) to get it into the typical hex logo format.

Not going to keep this on the readme, but want to hold onto the logo code until I put it somewhere else.    
s <- sticker(image_path, package="micro4R", p_size=15, p_family = "Comfortaa", p_fontface = "bold", p_y = 1.5, s_x=1, s_y=.75, s_width=.5, s_height = .5, p_color = "black", h_fill = "#6ed5f5", h_color= "#16bc93", h_size = 2, filename="inst/figures/imgfile.png").   


built R 4.5.1    
RStudio Version 2025.05.1+513 (2025.05.1+513)
macOS Sequoia Version 15.6.1 

# Acknowledgements

As mentioned above, I could not have done any of this without the benefit of what the work of others. These tools below were especially important:

* [mothur](https://mothur.org)   
* [qiime](https://qiime2.org)   
* [mgsat](https://github.com/andreyto/mgsat)   
* maaslin[2](https://huttenhower.sph.harvard.edu/maaslin/)/[3](https://huttenhower.sph.harvard.edu/maaslin3/)   
* [vegan](https://cran.r-project.org/web/packages/vegan/index.html)
* [tidyverse](https://tidyverse.tidyverse.org), especially [dplyr](https://dplyr.tidyverse.org) and [ggplot2](https://ggplot2.tidyverse.org). In addition to these packages, Hadley Wickham and team have written several extremely helpful [guides and tutorials](https://hadley.nz) on data science.
* [dada2 and decontam](https://callahanlab.cvm.ncsu.edu/software/)
* [Suite from Dr. Frank Harrell](https://hbiostat.org), especially [rms](https://cran.r-project.org/web/packages/rms/index.html) and [Hmisc](https://cran.r-project.org/web/packages/Hmisc/index.html).
* Colleague and physician-scientist [Dr. Christian Rosas-Salazar](https://pediatrics.vumc.org/person/christian-rosas-salazar-md-mph) is talented at many things, but has an especial knack for creating figures that are both beautiful and informative.   

More acknowledgements and more details to be added later
